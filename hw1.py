# -*- coding: utf-8 -*-
"""hw1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MBbbJr7D1w2kwmwbimk2rDlQOeX_6g6X
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import TensorDataset, DataLoader
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
import os
import random
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import SGD


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True

set_seed(52)


train_df = pd.read_csv("loan_train.csv")
test_df = pd.read_csv("loan_test.csv")

train_df.drop(columns=["id"], inplace=True)
test_df.drop(columns=["id"], inplace=True)

target_col = "loan_status"

X_train = train_df.drop(columns=[target_col])
y_train = train_df[target_col]

X_test = test_df.drop(columns=[target_col])
y_test = test_df[target_col]


cat_features = X_train.select_dtypes(include=["object"]).columns.tolist()
num_features = X_train.select_dtypes(exclude=["object"]).columns.tolist()


encoders = {}
embedding_sizes = {}

for col in cat_features:
    print(col, X_train[col].unique())
    le = LabelEncoder()
    full_col = pd.concat([X_train[col], X_test[col]], axis=0)
    le.fit(full_col)

    encoders[col] = le
    embedding_sizes[col] = len(le.classes_)

    X_train[col] = le.transform(X_train[col])
    X_test[col] = le.transform(X_test[col])

scaler = StandardScaler()
X_train[num_features] = scaler.fit_transform(X_train[num_features])
X_test[num_features] = scaler.transform(X_test[num_features])

X_train_cat = torch.tensor(X_train[cat_features].values, dtype=torch.long)
X_test_cat = torch.tensor(X_test[cat_features].values, dtype=torch.long)

X_train_num = torch.tensor(X_train[num_features].values, dtype=torch.float32)
X_test_num = torch.tensor(X_test[num_features].values, dtype=torch.float32)

y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)

train_dataset = TensorDataset(X_train_cat, X_train_num, y_train_tensor)
test_dataset = TensorDataset(X_test_cat, X_test_num, y_test_tensor)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)

"""# тренировочка"""

'cuda' if torch.cuda.is_available() else 'cpu'

class Trainer:
    def __init__(self, model, train_loader, test_loader,
                 learning_rate=0.01, weight_decay=0.0,
                 epochs=10, device='cuda' if torch.cuda.is_available() else 'cpu'):

        self.device = device
        self.model = model.to(self.device)
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.epochs = epochs

        self.criterion = nn.BCEWithLogitsLoss()
        self.optimizer = SGD(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)

        self.train_losses = []
        self.test_losses = []
        self.train_aucs = []
        self.test_aucs = []

    def train(self):
        for epoch in range(1, self.epochs + 1):
            train_loss, train_auc = self._train_one_epoch()
            test_loss, test_auc = self._evaluate()

            self.train_losses.append(train_loss)
            self.test_losses.append(test_loss)
            self.train_aucs.append(train_auc)
            self.test_aucs.append(test_auc)

            print(f"Epoch {epoch}/{self.epochs} | "
                  f"Train Loss: {train_loss:.4f}, AUC: {train_auc:.4f} | "
                  f"Test Loss: {test_loss:.4f}, AUC: {test_auc:.4f}")

    def _train_one_epoch(self):
        self.model.train()
        epoch_loss = 0
        preds, targets = [], []

        for x_cat, x_num, yb in self.train_loader:
            x_cat = x_cat.to(self.device)
            x_num = x_num.to(self.device)
            yb = yb.to(self.device)

            self.optimizer.zero_grad()
            logits = self.model(x_cat, x_num).squeeze(1)
            loss = self.criterion(logits, yb)
            loss.backward()
            self.optimizer.step()

            epoch_loss += loss.item() * x_cat.size(0)
            preds.extend(torch.sigmoid(logits).detach().cpu().numpy())
            targets.extend(yb.detach().cpu().numpy())

        epoch_loss /= len(self.train_loader.dataset)
        auc = roc_auc_score(targets, preds)
        return epoch_loss, auc

    def _evaluate(self):
        self.model.eval()
        epoch_loss = 0
        preds, targets = [], []

        with torch.no_grad():
            for x_cat, x_num, yb in self.test_loader:
                x_cat = x_cat.to(self.device)
                x_num = x_num.to(self.device)
                yb = yb.to(self.device)

                logits = self.model(x_cat, x_num).squeeze(1)
                loss = self.criterion(logits, yb)

                epoch_loss += loss.item() * x_cat.size(0)
                preds.extend(torch.sigmoid(logits).cpu().numpy())
                targets.extend(yb.cpu().numpy())

        epoch_loss /= len(self.test_loader.dataset)
        auc = roc_auc_score(targets, preds)
        return epoch_loss, auc

    def plot_metrics(self, filename_prefix=None):
        epochs_range = range(1, self.epochs + 1)

        plt.figure(figsize=(14, 5))

        # график Loss
        plt.subplot(1, 2, 1)
        plt.plot(epochs_range, self.train_losses, label="Train Loss")
        plt.plot(epochs_range, self.test_losses, label="Test Loss")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.title("Loss over Epochs")
        plt.legend()

        # график AUC
        plt.subplot(1, 2, 2)
        plt.plot(epochs_range, self.train_aucs, label="Train AUC")
        plt.plot(epochs_range, self.test_aucs, label="Test AUC")
        plt.xlabel("Epoch")
        plt.ylabel("AUC")
        plt.title("ROC-AUC over Epochs")
        plt.legend()

        plt.tight_layout()

        if filename_prefix:
            plt.savefig(f"{filename_prefix}_combined.png")
            print(f"Графики сохранены как '{filename_prefix}_combined.png'")

        plt.show()

def plot_all_experiments(experiments, mode='both', filename=None):
    plt.figure(figsize=(14, 5))

    # AUC
    plt.subplot(1, 2, 1)
    for exp in experiments:
        label = exp.get('label', 'unnamed')
        if mode in ['train', 'both']:
            plt.plot(exp['train_aucs'], label=f"{label} - Train AUC")
        if mode in ['test', 'both']:
            plt.plot(exp['test_aucs'], label=f"{label} - Test AUC")
    plt.title("AUC over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("ROC-AUC")
    plt.legend()

    # Loss
    plt.subplot(1, 2, 2)
    for exp in experiments:
        label = exp.get('label', 'unnamed')
        if mode in ['train', 'both']:
            plt.plot(exp['train_losses'], label=f"{label} - Train Loss")
        if mode in ['test', 'both']:
            plt.plot(exp['test_losses'], label=f"{label} - Test Loss")
    plt.title("Loss over Epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()

    plt.tight_layout()

    if filename:
        plt.savefig(f"{filename}.png")
        print(f"График {filename}.png")

    plt.show()

def export_experiment_data(experiment_info, output_file='experiment_results.csv'):
    epochs = list(range(1, len(experiment_info['train_losses']) + 1))
    data = {
        'epoch': epochs,
        'train_loss': experiment_info['train_losses'],
        'validation_loss': experiment_info['test_losses'],
        'train_auc_score': experiment_info['train_aucs'],
        'validation_auc_score': experiment_info['test_aucs']
    }

    results_df = pd.DataFrame(data)
    results_df.to_csv(output_file, index=False)
    print(f"Данные эксперимента в {output_file}")

def save_experiment_to_csv(experiment_dict, filename='experiment.csv'):
    df = pd.DataFrame({
        'epoch': list(range(1, len(experiment_dict['train_losses']) + 1)),
        'train_loss': experiment_dict['train_losses'],
        'test_loss': experiment_dict['test_losses'],
        'train_auc': experiment_dict['train_aucs'],
        'test_auc': experiment_dict['test_aucs']
    })

    df.to_csv(filename, index=False)
    print(f"Эксперимент в {filename}")

"""## Эксперимент 1"""

class Model1(nn.Module):
    def __init__(self, embedding_dimensions, numerical_input_dim, hidden_units=32, embedding_dimension=4):
        super(Model1, self).__init__()

        self.embedding_layers = nn.ModuleList([
            nn.Embedding(num_categories, embedding_dimension)
            for num_categories in embedding_dimensions.values()
        ])

        total_embedding_size = embedding_dimension * len(embedding_dimensions)

        self.initial_layer = nn.Linear(total_embedding_size + numerical_input_dim, hidden_units)

        self.hidden_block = nn.Sequential(
            nn.Linear(hidden_units, hidden_units * 4),
            nn.ReLU(),
            nn.Linear(hidden_units * 4, hidden_units)
        )

        self.final_layer = nn.Linear(hidden_units, 1)

    def forward(self, categorical_inputs, numerical_inputs):
        embedded_outputs = [layer(categorical_inputs[:, i]) for i, layer in enumerate(self.embedding_layers)]
        concatenated_embeddings = torch.cat(embedded_outputs, dim=1)

        combined_inputs = torch.cat([concatenated_embeddings, numerical_inputs], dim=1)

        x = self.initial_layer(combined_inputs)
        x = self.hidden_block(x)
        x = self.final_layer(x)
        return x

model = Model1(
    embedding_dimensions=embedding_sizes,
    numerical_input_dim=X_train_num.shape[1],
    hidden_units=32,
)

trainer = Trainer(
    model=model,
    train_loader=train_loader,
    test_loader=test_loader,
    learning_rate=0.01,
    epochs=50
)

trainer.train()

trainer.plot_metrics('experiment_1')

experiment_1 = {
    'test_aucs': trainer.test_aucs,
    'train_aucs': trainer.train_aucs,
    'test_losses': trainer.test_losses,
    'train_losses': trainer.train_losses,
    'label': "exp1"
    }
export_experiment_data(experiment_1, 'experiment_1.csv')
experiment_1

print(max( enumerate(experiment_1["test_aucs"]), key = lambda x: x[1] ))
print(min( enumerate(experiment_1["test_losses"]), key = lambda x: x[1] ))

"""## Эксперимент 2"""

class MLPUnit(nn.Module):
    def __init__(self, size):
        super(MLPUnit, self).__init__()
        self.layer = nn.Sequential(
            nn.Linear(size, size * 4),
            nn.ReLU(),
            nn.Linear(size * 4, size)
        )

    def forward(self, x):
        return self.layer(x)

class Model2(nn.Module):
    def __init__(self, category_sizes, numerical_input_size, hidden_layer_size=128, num_units=3, embedding_dimension=4):
        super(Model2, self).__init__()

        self.embedding_layers = nn.ModuleList([
            nn.Embedding(num_categories, embedding_dimension)
            for num_categories in category_sizes.values()
        ])

        total_embedding_dim = embedding_dimension * len(category_sizes)
        input_dimension = total_embedding_dim + numerical_input_size

        self.initial_layer = nn.Linear(input_dimension, hidden_layer_size)

        self.mlp_units = nn.Sequential(
            *[MLPUnit(hidden_layer_size) for _ in range(num_units)]
        )

        self.final_layer = nn.Linear(hidden_layer_size, 1)

    def forward(self, categorical_data, numerical_data):
        embedded_features = [embedding(categorical_data[:, i]) for i, embedding in enumerate(self.embedding_layers)]
        embedded_features = torch.cat(embedded_features, dim=1)
        combined_input = torch.cat([embedded_features, numerical_data], dim=1)

        x = self.initial_layer(combined_input)
        x = self.mlp_units(x)
        x = self.final_layer(x)
        return x

model = Model2(
    category_sizes=embedding_sizes,
    numerical_input_size=X_train_num.shape[1],
    hidden_layer_size=128,
    num_units=3,
    embedding_dimension=4
)

trainer = Trainer(
    model=model,
    train_loader=train_loader,
    test_loader=test_loader,
    learning_rate=0.01,
    weight_decay=0.0,
    epochs=45,
)

trainer.train()

trainer.plot_metrics('experiment_2')

experiment_2 = {
    'test_aucs': trainer.test_aucs,
    'train_aucs': trainer.train_aucs,
    'test_losses': trainer.test_losses,
    'train_losses': trainer.train_losses,
    'label': "exp2"
    }
export_experiment_data(experiment_2, 'experiment_2.csv')
experiment_2

print(max( enumerate(experiment_2["test_aucs"]), key = lambda x: x[1] ))
print(min( enumerate(experiment_2["test_losses"]), key = lambda x: x[1] ))

plot_all_experiments([experiment_1, experiment_2], 'test', 'compare_1_to_2')

"""## Эксперимент 3"""

class ResidualBlock(nn.Module):
    def __init__(self, feature_size):
        super(ResidualBlock, self).__init__()
        self.batch_norm = nn.BatchNorm1d(feature_size)
        self.fc1 = nn.Linear(feature_size, feature_size * 4)
        self.activation = nn.ReLU()
        self.fc2 = nn.Linear(feature_size * 4, feature_size)

    def forward(self, input_tensor):
        shortcut = input_tensor
        x = self.batch_norm(input_tensor)
        x = self.fc1(x)
        x = self.activation(x)
        x = self.fc2(x)
        return x + shortcut

class Model3(nn.Module):
    def __init__(self, category_sizes, numerical_dim, hidden_units=128, block_count=3, embedding_dim=4):
        super(Model3, self).__init__()
        self.embedding_layers = nn.ModuleList([
            nn.Embedding(num_categories, embedding_dim)
            for num_categories in category_sizes.values()
        ])

        total_embedding_dim = embedding_dim * len(category_sizes)
        input_dimension = total_embedding_dim + numerical_dim

        self.initial_layer = nn.Linear(input_dimension, hidden_units)

        self.residual_blocks = nn.Sequential(
            *[ResidualBlock(hidden_units) for _ in range(block_count)]
        )

        self.final_layer = nn.Linear(hidden_units, 1)

    def forward(self, categorical_input, numerical_input):
        embeddings = [layer(categorical_input[:, i]) for i, layer in enumerate(self.embedding_layers)]
        concatenated_embeddings = torch.cat(embeddings, dim=1)
        combined_input = torch.cat([concatenated_embeddings, numerical_input], dim=1)
        x = self.initial_layer(combined_input)
        x = self.residual_blocks(x)
        output = self.final_layer(x)
        return output

model = Model3(
    category_sizes=embedding_sizes,
    numerical_dim=X_train_num.shape[1],
    hidden_units=128,
    block_count=3,
    embedding_dim=4
)

trainer = Trainer(
    model=model,
    train_loader=train_loader,
    test_loader=test_loader,
    learning_rate=0.01,
    weight_decay=0.0,
    epochs=45,
)

trainer.train()

trainer.plot_metrics(filename_prefix="experiment_3")

experiment_3 = {
    'test_aucs': trainer.test_aucs,
    'train_aucs': trainer.train_aucs,
    'test_losses': trainer.test_losses,
    'train_losses': trainer.train_losses,
    'label': "exp3"
    }
export_experiment_data(experiment_3, 'experiment_3.csv')
experiment_3

plot_all_experiments([experiment_2, experiment_3], 'test', 'compare_2_to_3')

print(max( enumerate(experiment_3["test_aucs"]), key = lambda x: x[1] ))
print(min( enumerate(experiment_3["test_losses"]), key = lambda x: x[1] ))

"""## Эксперимент 4"""

class DropoutBlock(nn.Module):
    def __init__(self, hidden_size, dropout_rate=0.1):
        super(DropoutBlock, self).__init__()
        self.norm = nn.BatchNorm1d(hidden_size)
        self.linear1 = nn.Linear(hidden_size, hidden_size * 4)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout_rate)
        self.linear2 = nn.Linear(hidden_size * 4, hidden_size)

    def forward(self, x):
        residual = x
        out = self.norm(x)
        out = self.linear1(out)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.linear2(out)
        return out + residual

class Model4(nn.Module):
    def __init__(self, category_sizes, numerical_dim, hidden_units=128, block_count=3, dropout_rate=0.1, embedding_dim=4):
        super(Model4, self).__init__()
        self.embeddings = nn.ModuleList([
            nn.Embedding(num_categories, embedding_dim)
            for num_categories in category_sizes.values()
        ])

        total_emb_dim = embedding_dim * len(category_sizes)
        input_dim = total_emb_dim + numerical_dim

        self.input_layer = nn.Linear(input_dim, hidden_units)
        self.blocks = nn.Sequential(
            *[DropoutBlock(hidden_units, dropout_rate) for _ in range(block_count)]
        )
        self.output_layer = nn.Linear(hidden_units, 1)

    def forward(self, x_cat, x_num):
        x_emb = [emb_layer(x_cat[:, i]) for i, emb_layer in enumerate(self.embeddings)]
        x_emb = torch.cat(x_emb, dim=1)
        x = torch.cat([x_emb, x_num], dim=1)
        x = self.input_layer(x)
        x = self.blocks(x)
        x = self.output_layer(x)
        return x

dropout_rates = [0.01, 0.1, 0.2, 0.5, 0.9]
experiment_results = []

for p in dropout_rates:
    print(f"\nЗапуск модели с dropout = {p}")

    model = Model4(
        category_sizes=embedding_sizes,
        numerical_dim=X_train_num.shape[1],
        dropout_rate=p,
        hidden_units=128,
        block_count=3,
        embedding_dim=4
    )

    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        test_loader=test_loader,
        learning_rate=0.01,
        weight_decay=0.0,
        epochs=35,
        device='cuda'
    )

    trainer.train()
    prefix = f"experiment_4_dropout_{p}"

    trainer.plot_metrics(filename_prefix=prefix)

    experiment_dict = {
        'label': f'dropout_{p}',
        'train_losses': trainer.train_losses,
        'test_losses': trainer.test_losses,
        'train_aucs': trainer.train_aucs,
        'test_aucs': trainer.test_aucs
    }
    export_experiment_data(experiment_dict, f"{prefix}.csv")
    experiment_results.append(experiment_dict)

best = max(experiment_results, key=lambda x: x['test_aucs'][-1])
print(f"\nЛучший dropout_rate: {best['label']} — Test AUC: {best['test_aucs'][-1]:.4f}")

experiment_results

plot_all_experiments(experiment_results, 'test', 'compare_dropout')

"""## Эксперимент 5"""

learning_rates = [0.01, 0.05, 0.1]
weight_decays = [0.1, 0.01, 0.001]
dropout_rate = 0.5

experiment_5_results = []

for lr in learning_rates:
    for wd in weight_decays:
        print(f"\nОбучаем модель: lr={lr}, weight_decay={wd}")

        model = Model4(
            category_sizes=embedding_sizes,
            numerical_dim=X_train_num.shape[1],
            dropout_rate=dropout_rate,
            hidden_units=128,
            block_count=3,
            embedding_dim=4
        )

        trainer = Trainer(
            model=model,
            train_loader=train_loader,
            test_loader=test_loader,
            learning_rate=lr,
            weight_decay=wd,
            epochs=35,
            device='cuda'
        )

        trainer.train()
        prefix = f"experiment_5_lr_{lr}_wd_{wd}"

        trainer.plot_metrics(filename_prefix=prefix)

        experiment_dict = {
            'label': f'lr={lr}_wd={wd}',
            'train_losses': trainer.train_losses,
            'test_losses': trainer.test_losses,
            'train_aucs': trainer.train_aucs,
            'test_aucs': trainer.test_aucs
        }
        save_experiment_to_csv(experiment_dict, f"{prefix}.csv")
        experiment_5_results.append(experiment_dict)

best_exp = max(experiment_5_results, key=lambda x: x['test_aucs'][-1])
print(f"\nЛучшая комбинация: {best_exp['label']} — Test AUC: {best_exp['test_aucs'][-1]:.4f}")

model = Model4(
    category_sizes=embedding_sizes,
    numerical_dim=X_train_num.shape[1],
    dropout_rate=dropout_rate,
    embedding_dim=4,
    hidden_units=128,
    block_count=3
)

trainer = Trainer(
    model=model,
    train_loader=train_loader,
    test_loader=test_loader,
    learning_rate=0.05,
    weight_decay=0.001,
    epochs=45,
)

trainer.train()
prefix = f"experiment_5_best"

trainer.plot_metrics(filename_prefix=prefix)

plot_all_experiments(
    [experiment_2,
     {
         'label': f'exp5',
         'train_losses': trainer.train_losses,
         'test_losses': trainer.test_losses,
         'train_aucs': trainer.train_aucs,
         'test_aucs': trainer.test_aucs
     }
    ],
    'test',
    'compare_5_to_2'
)

print(max( enumerate(trainer.test_aucs), key = lambda x: x[1] ))
print(min( enumerate(trainer.test_losses), key = lambda x: x[1] ))

